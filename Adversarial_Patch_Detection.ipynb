{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj1tkvtRLe-B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the RESISC45 dataset into train, validation and test folder for training our ResNet50 model."
      ],
      "metadata": {
        "id": "qpnRIDSVLwrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "def split_dataset(dataset_dir, output_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    # Check if any of the ratio variables have been overwritten in the global scope\n",
        "    if 'train_ratio' in globals():\n",
        "        print(\"Warning: 'train_ratio' is defined in the global scope. Using the default value for splitting.\")\n",
        "    if 'val_ratio' in globals():\n",
        "        print(\"Warning: 'val_ratio' is defined in the global scope. Using the default value for splitting.\")\n",
        "    if 'test_ratio' in globals():\n",
        "        print(\"Warning: 'test_ratio' is defined in the global scope. Using the default value for splitting.\")\n",
        "\n",
        "    assert train_ratio + val_ratio + test_ratio == 1, \"Train, validation and test ratios must sum to 1\"\n",
        "    classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(dataset_dir, class_name)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        train_split = int(train_ratio * len(images))\n",
        "        val_split = int(val_ratio * len(images))\n",
        "\n",
        "        train_images = images[:train_split]\n",
        "        val_images = images[train_split:train_split + val_split]\n",
        "        test_images = images[train_split + val_split:]\n",
        "\n",
        "        # Create directories for train, validation, and test splits\n",
        "        train_dir = os.path.join(output_dir, 'train', class_name)\n",
        "        val_dir = os.path.join(output_dir, 'validation', class_name)\n",
        "        test_dir = os.path.join(output_dir, 'test', class_name)\n",
        "\n",
        "        os.makedirs(train_dir, exist_ok=True)\n",
        "        os.makedirs(val_dir, exist_ok=True)\n",
        "        os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "        # Move images to the corresponding directories\n",
        "        for img in tqdm(train_images, desc=f'Moving train images for class {class_name}'):\n",
        "            shutil.move(os.path.join(class_dir, img), os.path.join(train_dir, img))\n",
        "\n",
        "        for img in tqdm(val_images, desc=f'Moving validation images for class {class_name}'):\n",
        "            shutil.move(os.path.join(class_dir, img), os.path.join(val_dir, img))\n",
        "\n",
        "        for img in tqdm(test_images, desc=f'Moving test images for class {class_name}'):\n",
        "            shutil.move(os.path.join(class_dir, img), os.path.join(test_dir, img))\n",
        "\n",
        "# Example usage - Provide values for train_ratio, val_ratio, and test_ratio that sum to 1\n",
        "dataset_dir = '/content/drive/MyDrive/darpa/data_in'\n",
        "output_dir = '/content/drive/MyDrive/darpa/data_out'\n",
        "split_dataset(dataset_dir, output_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1) # Example: 80% train, 10% validation, 10% test"
      ],
      "metadata": {
        "id": "FmlxtaVVLn0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Validating the ResNet50 model with 45 classes RESISC45 dataset for finding the Class Consistency."
      ],
      "metadata": {
        "id": "5NXG1w5PMHD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Add, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "import os"
      ],
      "metadata": {
        "id": "vL8Nt3kDLz_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "nzyW1yEqMU73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    F1, F2, F3 = filters\n",
        "    X_shortcut = X\n",
        "\n",
        "    X = Conv2D(F1, (1, 1), name=conv_name_base + '2a')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(F2, (f, f), padding='same', name=conv_name_base + '2b')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(F3, (1, 1), name=conv_name_base + '2c')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    F1, F2, F3 = filters\n",
        "    X_shortcut = X\n",
        "\n",
        "    X = Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    #Add activation function\n",
        "\n",
        "    X = Conv2D(F2, (f, f), padding='same', name=conv_name_base + '2b')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    #Add activation function\n",
        "\n",
        "    X = Conv2D(F3, (1, 1), name=conv_name_base + '2c')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), name=conv_name_base + '1')(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    X = Add()([X, X_shortcut])\n",
        "    #Add activation function\n",
        "\n",
        "    return X\n",
        "\n",
        "def ResNet50(input_shape=(224, 224, 3), classes=45):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(X_input)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "   ##Add Global Average pooling and Desne layer for taining our model.\n",
        "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set mixed precision policy\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "# Create a MirroredStrategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                       shear_range=0.2,\n",
        "                                       zoom_range=0.2,\n",
        "                                       horizontal_flip=True)\n",
        "\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/darpa/data_out/train',\n",
        "                                                        target_size=(224, 224),\n",
        "                                                        batch_size=16,  # Reduce batch size\n",
        "                                                        class_mode='categorical')\n",
        "\n",
        "    validation_generator = val_datagen.flow_from_directory('/content/drive/MyDrive/darpa/data_out/validation',\n",
        "                                                           target_size=(224, 224),\n",
        "                                                           batch_size=16,  # Reduce batch size\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "    model = ResNet50(input_shape=(224, 224, 3), classes=train_generator.num_classes)\n",
        "    model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
        "\n",
        "    model.fit(train_generator,\n",
        "              steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "              validation_data=validation_generator,\n",
        "              validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "              epochs=20,\n",
        "              callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "    # Save the final model\n",
        "    model.save('/content/drive/MyDrive/darpa/resnet50_model1.h5')\n"
      ],
      "metadata": {
        "id": "5NPYjinvMkyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measure the accuracy of our Trained model with Testing dataset"
      ],
      "metadata": {
        "id": "yN-Um-S-M1eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/darpa/resnet50_model1.h5')\n",
        "\n",
        "# Prepare the test data generator\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/MyDrive/darpa/data_out/test',\n",
        "                                                  target_size=(224, 224),\n",
        "                                                  batch_size=16,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=False)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Predict the classes of the test data\n",
        "Y_pred = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(test_generator.classes, y_pred)\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(test_generator.classes, y_pred, target_names=class_names)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "TzES14tUMlt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find Class Consistency by predicting top 5 classes of provided image with our trained ResNet50 model"
      ],
      "metadata": {
        "id": "tCUC_nZKNGBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load your trained ResNet50 model\n",
        "model_path = \"/content/drive/MyDrive/darpa/resnet50_model1.h5\"  # Replace with your model path\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Load and preprocess the image\n",
        "file_path = \"/content/drive/MyDrive/darpa/findclass_level/patched_image_new_1424.png\"  # Replace with your image path\n",
        "img = image.load_img(file_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Normalize pixel values\n",
        "\n",
        "# Predict classes\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# Decode predictions manually for 45 classes\n",
        "\n",
        "class_names = [\n",
        "    'airplane', 'airport', 'baseball_diamond', 'basketball_court', 'beach',\n",
        "    'bridge', 'chaparral', 'church', 'circular_farmland', 'cloud',\n",
        "    'commercial_area', 'dense_residential', 'desert', 'forest', 'freeway',\n",
        "    'golf_course', 'ground_track_field', 'harbor', 'industrial_area', 'intersection',\n",
        "    'island', 'lake', 'meadow', 'medium_residential', 'mobile_home_park',\n",
        "    'mountain', 'overpass', 'palace', 'parking_lot', 'railway',\n",
        "    'railway_station', 'rectangular_farmland', 'river', 'roundabout', 'runway',\n",
        "    'sea_ice', 'ship', 'snowberg', 'sparse_residential', 'stadium',\n",
        "    'storage_tank', 'tennis_court', 'terrace', 'thermal_power_station', 'wetland'\n",
        "]\n",
        "# Predict top 5 classes of a given image with our trained model\n",
        "\n",
        "# Display top predictions\n",
        "for i, class_idx in enumerate(top_classes):\n",
        "    class_name = class_names[class_idx]\n",
        "    score = top_scores[i]\n",
        "    print(f\"{i + 1}: {class_name} ({score:.2f})\")"
      ],
      "metadata": {
        "id": "vcmufh_OM1UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our trained model divide the images in two folder which one is attacked and which are not based on class consistency."
      ],
      "metadata": {
        "id": "Q9nm5yiYglXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Load your trained ResNet50 model\n",
        "model_path = \"/content/drive/MyDrive/darpa/resnet50_model1.h5\"  # Replace with your model path\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Paths to the folders containing images and where to move classified images\n",
        "image_folder = \"/content/drive/MyDrive/darpa/test/terrace\"  # Replace with your folder path\n",
        "attacked_folder = \"/content/drive/MyDrive/darpa/attackedd/\"  # Replace with your folder path\n",
        "non_attacked_folder = \"/content/drive/MyDrive/darpa/non_attackedd/\"  # Replace with your folder path\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(attacked_folder, exist_ok=True)\n",
        "os.makedirs(non_attacked_folder, exist_ok=True)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\n",
        "    'airplane', 'airport', 'baseball_diamond', 'basketball_court', 'beach',\n",
        "    'bridge', 'chaparral', 'church', 'circular_farmland', 'cloud',\n",
        "    'commercial_area', 'dense_residential', 'desert', 'forest', 'freeway',\n",
        "    'golf_course', 'ground_track_field', 'harbor', 'industrial_area', 'intersection',\n",
        "    'island', 'lake', 'meadow', 'medium_residential', 'mobile_home_park',\n",
        "    'mountain', 'overpass', 'palace', 'parking_lot', 'railway',\n",
        "    'railway_station', 'rectangular_farmland', 'river', 'roundabout', 'runway',\n",
        "    'sea_ice', 'ship', 'snowberg', 'sparse_residential', 'stadium',\n",
        "    'storage_tank', 'tennis_court', 'terrace', 'thermal_power_station', 'wetland'\n",
        "]\n",
        "\n",
        "# Threshold to determine if top class probabilities are almost the same\n",
        "probability_threshold = 0.95\n",
        "\n",
        "# Process each image in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    file_path = os.path.join(image_folder, filename)\n",
        "    if not os.path.isfile(file_path):\n",
        "        continue\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(file_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Normalize pixel values\n",
        "\n",
        "\n",
        "    # Predict top 5 classes of a given image with our trained model and\n",
        "    #divide the images into attacked and non-attacked folder\n",
        "\n",
        "\n",
        "    # Determine if the image is attacked or non-attacked\n",
        "    attacked = False\n",
        "    for i in range(1, 3):  # Compare top class with 2nd and 3rd classes\n",
        "        if abs(top_scores[0] - top_scores[i]) < probability_threshold:\n",
        "            attacked = True\n",
        "            break\n",
        "\n",
        "    status = \"attacked\" if attacked else \"non-attacked\"\n",
        "\n",
        "    # Move the image to the corresponding folder\n",
        "    if status == \"attacked\":\n",
        "        shutil.move(file_path, os.path.join(attacked_folder, filename))\n",
        "    else:\n",
        "        shutil.move(file_path, os.path.join(non_attacked_folder, filename))\n",
        "\n",
        "    # Display result\n",
        "    print(f\"Image: {filename}\")\n",
        "    print(\"Top 5 predicted classes:\")\n",
        "    for i, class_idx in enumerate(top_classes):\n",
        "        class_name = class_names[class_idx]\n",
        "        score = top_scores[i]\n",
        "        print(f\"{i + 1}: {class_name} ({score:.2f})\")\n",
        "    print(f\"Status: {status}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "naZgZIo5NWHw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}